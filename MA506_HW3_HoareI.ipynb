{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL-_Pq5CB0a8"
      },
      "outputs": [],
      "source": [
        "#installs and imports\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.linear_model import RidgeCV, Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LassoCV, Lasso\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = datasets.load_diabetes(return_X_y = False, as_frame = False, scaled = True)\n",
        "print(db.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zcYl3rIJq6J",
        "outputId": "36041c25-8b48-4755-c0e1-494c4fa0ee12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Preprocessing"
      ],
      "metadata": {
        "id": "zQzXFPspB3n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1\n",
        "\n",
        "Divide the dataset into 2 parts: $D_{tr}$ (training set), and $D_{te}$ (testing set) by randomly placing $80\\%$ of the data into $D_{tr}$ and $20\\%$ in $D_{te}$. For this, use train_test_split() from sklearn with a random state of $0$\n"
      ],
      "metadata": {
        "id": "AkDFx-0kCjr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use train_test_split\n",
        "D = db.data\n",
        "Y = db.target\n",
        "D_tr, D_te, Y_tr, Y_te = train_test_split(D, Y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#Turn D_tr and D_te into dataframes\n",
        "#D_tr = pd.DataFrame(data = D_tr, columns = db['feature_names'])\n",
        "#D_tr['target'] = Y_tr\n",
        "\n",
        "#D_te = pd.DataFrame(data = D_te, columns = db['feature_names'])\n",
        "#D_te['target'] = Y_te3"
      ],
      "metadata": {
        "id": "ZPtAikybKR9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check size of training set and testing set to ensure accuracy\n",
        "print('D_tr shape:', D_tr.shape)\n",
        "print('D_te shape:', D_te.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3smQc1VfMHxB",
        "outputId": "abc02496-22e5-4325-d8f7-ccc12f72c85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D_tr shape: (353, 10)\n",
            "D_te shape: (89, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Linear Regression"
      ],
      "metadata": {
        "id": "Tkcw4wHQDri1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1\n",
        "\n",
        "Divide $D_{tr}$ further into 2 different sets: $D_{train}$ (training set) and $D_{val}$ (validation set). Place $80\\%$ data in $D_{train}$ and remaining in $D_{val}$. Again use random state $0$."
      ],
      "metadata": {
        "id": "jiazYWAoDvOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use train_test_split\n",
        "D_train, D_val, Y_train, Y_val = train_test_split(D_tr, Y_tr, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#check size of arrays to ensure accuracy\n",
        "print('D_train shape:', D_train.shape)\n",
        "print('D_val shape:', D_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wk7ZKNKMzI2",
        "outputId": "bcb9e46a-6ca1-43ba-dfae-b5e4d4ec38e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D_train shape: (282, 10)\n",
            "D_val shape: (71, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2\n",
        "\n",
        "Fit the following 4 models separately on $D_{train}$.\n",
        "\n",
        "(a) $pred = \\beta_0 + \\beta_1 \\cdot bmi + \\beta_2 \\cdot bp$\n",
        "\n",
        "(b) $pred = \\beta_0 + \\beta_1 \\cdot bmi + \\beta_2 \\cdot s5$\n",
        "\n",
        "(c) $pred = \\beta_0 + \\beta_1 \\cdot bp + \\beta_2 \\cdot s5$\n",
        "\n",
        "(d) $pred = \\beta_0 + \\beta_1 \\cdot bmi + \\beta_2 \\cdot bp + \\beta_3 \\cdot s5$\n",
        "\n",
        "\n",
        "and print the $R^2$ value for each of these models on both $D_{train}$ and $D_{val}$."
      ],
      "metadata": {
        "id": "2U2V4DV0EKev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Solve using other methods (not preexisting trained sklearn model)\n",
        "#concatenate the features for the models\n",
        "D_a = np.concatenate((np.ones(len(D_train)).reshape(-1,1), D_train[:,2].reshape(-1,1), D_train[:,3].reshape(-1,1)), axis = 1)\n",
        "D_a_val = np.concatenate((np.ones(len(D_val)).reshape(-1,1), D_val[:,2].reshape(-1,1), D_val[:,3].reshape(-1,1)), axis = 1)\n",
        "\n",
        "D_b = np.concatenate((np.ones(len(D_train)).reshape(-1,1), D_train[:,2].reshape(-1,1), D_train[:,8].reshape(-1,1)), axis = 1)\n",
        "D_b_val = np.concatenate((np.ones(len(D_val)).reshape(-1,1), D_val[:,2].reshape(-1,1), D_val[:,8].reshape(-1,1)), axis = 1)\n",
        "\n",
        "D_c = np.concatenate((np.ones(len(D_train)).reshape(-1,1), D_train[:,3].reshape(-1,1), D_train[:,8].reshape(-1,1)), axis = 1)\n",
        "D_c_val = np.concatenate((np.ones(len(D_val)).reshape(-1,1), D_val[:,3].reshape(-1,1), D_val[:,8].reshape(-1,1)), axis = 1)\n",
        "\n",
        "D_d = np.concatenate((np.ones(len(D_train)).reshape(-1,1), D_train[:,2].reshape(-1,1), D_train[:,3].reshape(-1,1), D_train[:,8].reshape(-1,1)), axis = 1)\n",
        "D_d_val = np.concatenate((np.ones(len(D_val)).reshape(-1,1), D_val[:,2].reshape(-1,1), D_val[:,3].reshape(-1,1), D_val[:,8].reshape(-1,1)), axis = 1)\n",
        "\n",
        "#calculate coeffecients (betas)\n",
        "def coeffecients(X, Y):\n",
        "  XtX = np.dot(X.T, X)\n",
        "  XtY = np.dot(X.T, Y)\n",
        "  beta = np.linalg.solve(XtX, XtY)\n",
        "  return beta\n",
        "\n",
        "#calculate R^2 score\n",
        "def find_r_squared(Y_true, Y_pred):\n",
        "  mean_Y_true = np.mean(Y_true)\n",
        "  sst = np.sum((Y_true - mean_Y_true) ** 2) #total sum squares\n",
        "  ssr = np.sum((Y_true - Y_pred) ** 2) #sum-squared regression\n",
        "  r2 = 1 - (ssr / sst)\n",
        "  return r2\n",
        "\n",
        "#define features and results\n",
        "features = [('(a)', D_a), ('(b)', D_b), ('(c)', D_c), ('(d)', D_d)]\n",
        "results = {}\n",
        "\n",
        "#create loop for results\n",
        "for model_name, D_model in features:\n",
        "  beta_model = coeffecients(D_model, Y_train)\n",
        "  pred_train = np.dot(D_model, beta_model)\n",
        "  pred_val = np.dot(D_a_val if model_name == '(a)' else\n",
        "                    D_b_val if model_name == '(b)' else\n",
        "                    D_c_val if model_name == '(c)' else\n",
        "                    D_d_val, beta_model)\n",
        "  r2_train = find_r_squared(Y_train, pred_train)\n",
        "  r2_val = find_r_squared(Y_val, pred_val)\n",
        "  results[model_name] = {'beta': beta_model, 'r2_train': r2_train, 'r2_val': r2_val}\n",
        "\n",
        "#print results\n",
        "for model_name, result in results.items():\n",
        "  print(f\"{model_name} R^2 on D_train: {result['r2_train']:.6f}\")\n",
        "  print(f\"{model_name} R^2 on D_val: {result['r2_val']:.6f}\\n\")"
      ],
      "metadata": {
        "id": "JMQn5uqUQnqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b95cdcd-d4b8-48a7-cdf4-ae8eb3028b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(a) R^2 on D_train: 0.413425\n",
            "(a) R^2 on D_val: 0.379782\n",
            "\n",
            "(b) R^2 on D_train: 0.466246\n",
            "(b) R^2 on D_val: 0.575697\n",
            "\n",
            "(c) R^2 on D_train: 0.415429\n",
            "(c) R^2 on D_val: 0.257030\n",
            "\n",
            "(d) R^2 on D_train: 0.500763\n",
            "(d) R^2 on D_val: 0.489141\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3\n",
        "\n",
        "Choose the best model based on your analysis from previous part and fit that model on $D_{tr}$ and and display the $R^2$ on $D_{tr}$ and $D_{te}$."
      ],
      "metadata": {
        "id": "KF-AeMPhFfy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results above, I find that the best fit model is that which has the highest R^2 value for the validation set. I also want to avoid models where the R^2 for the training set is higher than the validation set, because that indicates to me that the model is overfitting. Based on this, I immediately see that all of the models, except for (b), are potentially overfitting. Model (b) also has the highest R^2 value for the validation set, so I conclude that model (b) is the best model."
      ],
      "metadata": {
        "id": "ItX305F1Gqoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_b_tr = np.concatenate((np.ones(len(D_tr)).reshape(-1,1), D_tr[:,2].reshape(-1,1), D_tr[:,8].reshape(-1,1)), axis = 1)\n",
        "D_b_te = np.concatenate((np.ones(len(D_te)).reshape(-1,1), D_te[:,2].reshape(-1,1), D_te[:,8].reshape(-1,1)), axis = 1)\n",
        "\n",
        "beta = coeffecients(D_b_tr, Y_tr)\n",
        "\n",
        "pred_tr = np.dot(D_b_tr, beta)\n",
        "pred_te = np.dot(D_b_te, beta)\n",
        "r2_tr = find_r_squared(Y_tr, pred_tr)\n",
        "r2_te = find_r_squared(Y_te, pred_te)\n",
        "\n",
        "print(\"(b) R^2 on D_tr: {:.6f}\".format(r2_tr))\n",
        "print(\"(b) R^2 on D_te: {:.6f}\".format(r2_te))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J7ApCf82TWN",
        "outputId": "4e41237b-caf7-4352-8e82-5620fe7f3ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(b) R^2 on D_tr: 0.496123\n",
            "(b) R^2 on D_te: 0.283540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4\n",
        "\n",
        "Explain the problems with the model selection approach you implemented in part 1, 2, and 3 of this question."
      ],
      "metadata": {
        "id": "AGOnsEb9GR1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly, with each step of this question, I learned a lot about splitting datasets, using the test_train_split() function. When implementing the models a, b, c, and d, when performing this on the D_train and D_val sets, model b fit the data best, having a higher R^2 value for the D_val vs the D_train, which is a sign of a good fitting model. When implemented on D_tr and D_te however, the original data that was split, the model performed significantly worse for D_te.\n"
      ],
      "metadata": {
        "id": "9mfe_mwehOR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: Ridge Regression\n",
        "\n",
        "For the linear model:\n",
        "\n",
        "> $pred = \\beta_0 + \\beta_1 \\cdot bmi + \\beta_2 \\cdot bp + \\beta_3 \\cdot s5$\n",
        "\n",
        "with $X$ and $Y$ containing the appropriate data, we now wish to fit a ridge regression model which has the following optimal weights:\n",
        "\n",
        "> $\\hat\\beta^{ridge} = (X^TX+n\\lambda I)^{-1}X^TY$\n",
        "\n",
        "Hence for fixed $X$ and $Y$ matrices, the problem of fitting a ridge regression model boils down to finding the right $\\lambda$. In this regard:"
      ],
      "metadata": {
        "id": "mWNam3RBGa57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1\n",
        "\n",
        "Use Generalized Cross Validation (CV) metric to compute and display the best $\\lambda$ on $D_{tr}$."
      ],
      "metadata": {
        "id": "GYQwuD13HUpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#utilizing model (d) on D_tr\n",
        "D_d_tr = np.concatenate((np.ones(len(D_tr)).reshape(-1,1), D_tr[:,2].reshape(-1,1), D_tr[:,3].reshape(-1,1), D_tr[:,8].reshape(-1,1)), axis = 1)\n",
        "\n",
        "#GCV Function provided in lectures\n",
        "def GCV(lam,X,y):\n",
        "    beta = np.linalg.inv(X.T.dot(X) + X.shape[0]*lam*np.eye(X.shape[1])).dot(X.T.dot(y))\n",
        "    yhat = X.dot(beta)\n",
        "    H = X.dot(np.linalg.inv(X.T.dot(X)).dot(X.T))\n",
        "    h = np.mean(np.diag(H))\n",
        "    s = 0\n",
        "    for i in range(len(y)):\n",
        "        term = (y[i] - yhat[i])/(1 - h)\n",
        "        s = s + term**2\n",
        "    return s/len(y)\n",
        "\n",
        "lam = np.linspace(1.0e-10, 10, 100)\n",
        "gcv = [GCV(i, D_d_tr, Y_tr) for i in lam]\n",
        "\n",
        "#optimize lambda for GCV\n",
        "def optimize_lamGCV(X,y,lam):\n",
        "    args = (X,y)\n",
        "    bnds = [(1.0e-12, None)]\n",
        "    lamb = [lam]\n",
        "    lam = minimize(GCV,lamb,args,bounds=bnds,method='SLSQP')\n",
        "    return lam\n",
        "\n",
        "#show lambda\n",
        "lam = optimize_lamGCV(D_d_tr,Y_tr,lam = 0.1)\n",
        "lam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i5Rm53JFvD3",
        "outputId": "9657d27a-5f4a-42ce-c4b4-0316e4f1d6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " message: Optimization terminated successfully\n",
              " success: True\n",
              "  status: 0\n",
              "     fun: 3073.5000572849535\n",
              "       x: [ 1.000e-12]\n",
              "     nit: 2\n",
              "     jac: [ 3.278e+00]\n",
              "    nfev: 4\n",
              "    njev: 2"
            ]
          },
          "metadata": {},
          "execution_count": 491
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lam.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln_BmJLyGK-H",
        "outputId": "48a6fc5f-bc26-47e1-e6d4-47c7c1765144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.00000563e-12])"
            ]
          },
          "metadata": {},
          "execution_count": 492
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print results\n",
        "best_lambda = lam.x\n",
        "print('The optimal lambda value for D_tr is:', best_lambda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxXSNXAVGkMT",
        "outputId": "d08539e1-0b9f-4966-820a-0ef1c586f627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal lambda value for D_tr is: [1.00000563e-12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2\n",
        "\n",
        "Using the best obtained $\\lambda$, compute and display the corresponding $\\hat \\beta^{ridge}$ on $D_{tr}$."
      ],
      "metadata": {
        "id": "JHkd9lJuHfYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(Y_tr)\n",
        "\n",
        "#determine beta coeff using ridge regression\n",
        "beta_ridge = np.linalg.inv(D_d_tr.T.dot(D_d_tr) + n*(np.exp(-12))*np.eye(D_d_tr.shape[1])).dot(D_d_tr.T.dot(Y_tr))\n",
        "\n",
        "#print results\n",
        "coeff_titles = ['β0 ridge', 'β1 ridge', 'β2 ridge', 'β3 ridge']\n",
        "\n",
        "#print coeffecients with titles\n",
        "for title, coeffecient in zip (coeff_titles, beta_ridge):\n",
        "  print(f'{title}: {coeffecient}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAbfni4FGwEa",
        "outputId": "e7952e2d-c0e0-468e-9ea7-10fcd1224a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "β0 ridge: 152.17875866243705\n",
            "β1 ridge: 636.8218182537266\n",
            "β2 ridge: 224.00347504875617\n",
            "β3 ridge: 559.929329152485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3\n",
        "\n",
        "Using $\\hat \\beta^{ridge}$ from previous part, compute and display $R^2$ on $D_{te}$."
      ],
      "metadata": {
        "id": "aWvMqhMZHy1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_d_te= np.concatenate((np.ones(len(D_te)).reshape(-1,1),D_te[:,2].reshape(-1,1), D_te[:,3].reshape(-1,1), D_te[:,8].reshape(-1,1)), axis = 1)\n",
        "\n",
        "#Continuing from part 3.2\n",
        "Y_ridge = D_d_te.dot(beta_ridge)\n",
        "r2 = find_r_squared(Y_te, Y_ridge)\n",
        "\n",
        "#print result\n",
        "print(f'R^2 on D_te: {r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vFzraJ7He9A",
        "outputId": "51e14b1b-e14a-4cd7-bb44-cc840f4f13a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 on D_te: 0.33378780900313443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: Lasso Regression\n",
        "\n",
        "Again for the linear model:\n",
        "\n",
        "> $pred = \\beta_0 + \\beta_1 \\cdot bmi + \\beta_2 \\cdot bp + \\beta_3 \\cdot s5$\n",
        "\n",
        "with $X$ and $Y$ containing the appropriate data, we now wish to fit a lasso regression model. Here again the objective of fitting a lasso regression model boils down to finding the right $\\lambda$. In this regard:"
      ],
      "metadata": {
        "id": "hYOfds5PIEiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1\n",
        "\n",
        "On $D_{tr}$, Use LassoCV from sklearn (again use a random state of 0) to do a K-fold cross validation base selection of best $\\lambda$. Display the best $\\lambda$ you obtain. Use $K = 5$ here."
      ],
      "metadata": {
        "id": "sehY7c6vIH56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphas = np.logspace(1.0e-10, 10, 100)\n",
        "lasso_cv = LassoCV(alphas=alphas, cv=KFold(n_splits=5))\n",
        "\n",
        "#fit lassocv to model D_tr\n",
        "lasso_cv.fit(D_d_tr, Y_tr)\n",
        "\n",
        "#print best lambda\n",
        "best_alpha = lasso_cv.alpha_\n",
        "print(\"The optimal lambda for D_tr is:\", best_alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s2mNKd9XNKf",
        "outputId": "e546ebc9-f872-4af3-ec93-3542b3693b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal lambda for D_tr is: 1.0000000002302585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2\n",
        "\n",
        "Using the best obtained $\\lambda$ in part 1 above, compute the predictions on $D_{te}$ and display the corresponding $R^2$ on $D_{te}$."
      ],
      "metadata": {
        "id": "gTF6FBg9I9wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fit lasso model based on best_alpha\n",
        "lasso = Lasso(alpha = best_alpha)\n",
        "lasso.fit(D_d_tr, Y_tr)\n",
        "\n",
        "#compute predictions and find r-squared\n",
        "pred = lasso.predict(D_d_te)\n",
        "r2 = find_r_squared(Y_te, pred)\n",
        "\n",
        "#Print results\n",
        "print(\"R^2 on D_te:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uipUuKabcONi",
        "outputId": "398dbf59-ca08-4dce-d274-c45d3beacfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 on D_te: 0.2613246825079295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3\n",
        "\n",
        "Comment on the relative performance of linear regression, ridge regression and lasso regression for this problem."
      ],
      "metadata": {
        "id": "Sppgq9_IJRo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For linear regression, I think there's interesting results in the rigidity of the models based on the training data vs the validation data. When a model works really well for one set of train_test_split data from the same dataset, when applied to another set of that data from another perspective, the model does not perform as well.\n",
        "\n",
        "For ridge regression, the model was more difficult to interpret and create, from my personal view. However, tbe R^2 value on the testing data was higher than that of the Lasso Regression model, leading me to believe that although Lasso Regression was easier to develop, the Ridge Regression model seems to fit the particular linear function better between the models. Linear regression can give us a peak at what we are directly looking at, but when applied to the testing data, the R^2 value lowers, showing a disconnect between those steps.\n",
        "\n",
        "For this model I would most likely choose Ridge Regression as the best model."
      ],
      "metadata": {
        "id": "3e7dlKeOgReZ"
      }
    }
  ]
}